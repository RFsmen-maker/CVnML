{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4ce11132",
   "metadata": {},
   "source": [
    "2022 级周学习记录报告\n",
    "===================================\n",
    "\n",
    "<font face='宋体' size = 20>\n",
    "<table>\n",
    "    <tr>\n",
    "        <td><p align='left'>时间：2022.07.18 - 2022.07.24</p></td>\n",
    "        <td><p align='left'>报告人：姜伟鹏</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p align='left'>审阅人：陈熠金</p></td>\n",
    "        <td><p align='left'>导师：梁 云</p></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><p align='left'>月计划：  \n",
    "\n",
    "（1） 完成数字图像处理学习与 python 代码实践；  \n",
    "（2） 完成机器学习方面基础理论学习与基本实践；  \n",
    "（3） 在 Ubuntu 系统使用 pytorch 进行简单的机器学习算法的验证；  \n",
    "（4） 分步实现如下：  \n",
    "第一周：数字图像的数据表示、边缘计算等；机器学习基本概念、计算机实验环境配置；  \n",
    "第二周：图像修复、噪声处理等；线性回归、误差处理；  \n",
    "第三周：图像分割相关理论；Gradient Descent；  \n",
    "第四周：图像偏微分运算相关理论；深度学习优化方法；  \n",
    "        </p></td>\n",
    "        <td><p align='left'>本周环节：  \n",
    "\n",
    "（1） 微分几何基础及其在图像处理的应用。  \n",
    "（2） 各种学习率优化算法与发展概况。  \n",
    "下周计划：继续学习基础理论，尝试使用本周所学微分几何工具进行图像处理，继续使用 pytorch 进行机器学习实验；  \n",
    "        </p></td>\n",
    "    </tr>\n",
    "</table>\n",
    "</font>\n",
    "\n",
    "所有资料、文件都在：\n",
    "[我的 Github](https://github.com/RFsmen-maker/CVnML \"Github: https://github.com/RFsmen-maker/CVnML\") \n",
    "和\n",
    "[国内镜像仓库](https://rfsmen.coding.net/p/cvnml \"CODING: https://rfsmen.coding.net/p/cvnml\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbe9eb2d",
   "metadata": {},
   "source": [
    "Part 1：图像处理——微分方程（PDEs）\n",
    "====================================\n",
    "&emsp;&emsp;图像实际上是连续的，但经过了数字化采样后成为了离散的，采样非常快所以感知是连续的。传统的图像处理大部分基于离散化的数学，但得益于计算机的发展和数学家们的推动，可以通过**数值分析（numerical analysis）**的方法将离散域中的图片转换到连续域进行分析，让计算精度取决于可用的计算资源，而不由算法限制，为高精度技术保留可能性。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebde2017",
   "metadata": {},
   "source": [
    "# 微分几何基础\n",
    "&emsp;&emsp;物体的轮廓是一种曲线，因此可以抽象化为曲线进行分析。\n",
    "\n",
    "* 令 $C(p) = \\{x(p), y(p)\\} ,\\quad p \\in [0,1]$，$p$ 取值范围归一化，其一点对应平面曲线上一点\n",
    "\n",
    "![pic](pics/basic_1.png \"(x,y) to P\")\n",
    "\n",
    "* 一个平面曲线存在：**切线（Tangent）、法线（Normal）、曲率（Curvature）**\n",
    "* $s$：弧长。\n",
    "* $C_p$：$C$ 对 $p$ 的导数。  \n",
    "* $C_s$：一个单位长度的切线，即 $|C_s| = 1$。  \n",
    "* $C_{ss}$：切线二阶导，与切向垂直，其模即曲率 $\\kappa$。  \n",
    "\n",
    "![pic](pics/basic_2.png \"(x,y) to P\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d5e71",
   "metadata": {},
   "source": [
    "## 线性变换\n",
    "\n",
    "* 仿射（Affine）：$\\{\\tilde{x},\\tilde{y}\\} = A\\{x,y\\}^T + \\bar{b}$，类似于对拉伸、倾斜投影的建模。\n",
    "\n",
    "$$\n",
    "I_2(x,y) = I_1(T_1(x,y), T_2(x,y))\n",
    "\\\\\n",
    "\\\\\n",
    "\\begin{pmatrix}\n",
    "T_1(x,y)\n",
    "\\\\\n",
    "T_2(x,y)\n",
    "\\end{pmatrix}\n",
    "=\n",
    "\\begin{pmatrix}\n",
    "a & b\n",
    "\\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "\\begin{pmatrix}\n",
    "x\n",
    "\\\\\n",
    "y\n",
    "\\end{pmatrix}\n",
    "+\n",
    "\\begin{pmatrix}\n",
    "e\n",
    "\\\\\n",
    "f\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "* 欧几里得（Euclidean）：$A = [\\bar{u}_1,\\bar{u}_2], <\\bar{u}_1,\\bar{u}_2>=0 and <\\bar{u}_i,\\bar{u}_i>=0$，对旋转、平移的建模，矩阵各列正交，内积为 0。\n",
    "\n",
    "![pic](pics/affine_euclidean.png \"Affine & Euclidean\")\n",
    "\n",
    "* 等仿射（Equi-Affine）：$\\{\\tilde{x},\\tilde{y}\\} = A\\{x,y\\}^T + \\bar{b}, det(A) = 1$，仿射变换中矩阵行列式为 1 时，成为等仿射变换。变换后面积不变。\n",
    "\n",
    "$$\n",
    "det\n",
    "\\begin{pmatrix}\n",
    "a & b\n",
    "\\\\\n",
    "c & d\n",
    "\\end{pmatrix}\n",
    "= 1\n",
    "$$\n",
    "\n",
    "![pic](pics/equi_affine.png \"Equi_Affine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e362c2ec",
   "metadata": {},
   "source": [
    "## 不变量（invariant）\n",
    "&emsp;&emsp;偏微分推导过程略。\n",
    "\n",
    "* 欧几里得变换：一个 弧长-曲率坐标系 决定唯一一个形状，即确定一个原点 $S$ 时，如果两个图形可以通过欧几里得运动联系起来，那么他们的曲率是一致的。\n",
    "\n",
    "![pic](pics/Euclidean_invariant.png \"Invariant of Euclidean\")\n",
    "\n",
    "* 仿射变换：一个 仿射弧长-仿射曲率 决定唯一一个投影物，仿射曲率 $\\mu$ 不变。\n",
    "\n",
    "![pic](pics/Affine_invariant.png \"Invariant of Affine\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89b9b4b1",
   "metadata": {},
   "source": [
    "## 三维空间曲面\n",
    "&emsp;&emsp;二维曲线可以拓展到三维曲面，曲面上一点有无数条曲线穿过，对于这一点的曲率，可以通过计算这些曲线的曲率定义，存在最大和最小的曲率，可以取平均曲率，也可以使用高斯曲率（最大与最小曲率的乘积）。\n",
    "\n",
    "![pic](pics/surface.png \"Surface\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7863fda6",
   "metadata": {},
   "source": [
    "# 曲线演化（Curve Evolution）\n",
    "&emsp;&emsp;利用微分几何，可以对平面曲线进行曲线演化。曲线上一点的向量可以投影成切向和法向，只有法向对曲线的形状有影响，切向只影响经过整段曲线的速度，因此法线上的形变量与点的曲率成比例。\n",
    "\n",
    "* 欧几里得：对于任意一个简单曲线，最终都可以收缩成一个圆点。\n",
    "\n",
    "$$\n",
    "C_t = \\kappa \\vec{n} ,\\quad C_t = C_{ss} \\, where \\, C_{ss} = \\kappa \\vec{n}\n",
    "$$\n",
    "\n",
    "![pic](pics/curvature_flow.png \"Curvature flow\")\n",
    "\n",
    "* 仿射：对于任意一个简单曲线，最终都可以收缩成一个椭圆点。\n",
    "\n",
    "$$\n",
    "C_t = \\kappa^{1/3} \\vec{n} ,\\quad C_t = <C_{vv},\\vec{n}>\\vec{n} \\, where \\, <C_{vv},\\vec{n}> = \\kappa^{1/3}\n",
    "$$\n",
    "\n",
    "![pic](pics/affine_flow.png \"Affine heat\")\n",
    "\n",
    "* 常量：以曲线上的每个点为圆心，作相同半径的圆。霍根斯原理（Huygens Principle），与没有障碍的情况下光的传播类似，可以改变曲线的拓扑结构，将曲线分离或“折叠”起来，可以用于图像锐化。\n",
    "\n",
    "$$\n",
    "C_t = \\vec{n}\n",
    "$$\n",
    "\n",
    "![pic](pics/constant_flow.png \"Constant flow\")\n",
    "![pic](pics/constant_flow_.png \"Constant flow\")\n",
    "\n",
    "* 曲线形状只与法向有关，因此各种变换可以总结为：\n",
    "\n",
    "$$\n",
    "C_t = V \\vec{n}\n",
    "$$\n",
    "\n",
    "* 存在导数：可以用于计算变换的关键点\n",
    "    * Length: $L_t = -\\int_0^L \\kappa V ds$  \n",
    "    * Area: $A_t = \\int_0^L Vds$  \n",
    "    * Curvature: $\\kappa_t = V_{ss} + \\kappa^2V$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2b5489",
   "metadata": {},
   "source": [
    "# 水平集（Level Sets）\n",
    "&emsp;&emsp;将曲线表达为：平面上所有使得特定方程等于 0 的点，对于一个图像（曲线），不需要知道具体的曲线方程，通过卷积运算分别对 $x, y$ 方向求导，再运用微分几何运算。\n",
    "\n",
    "$$\n",
    "C = \\{(x,y)|\\Phi(x,y) = 0\\}\n",
    "\\\\\n",
    "\\vec{N} = - \\frac{\\nabla \\Phi}{|\\nabla \\Phi|}\n",
    "\\\\\n",
    "...\n",
    "\\\\\n",
    "\\Phi_t = V|\\nabla \\Phi|\n",
    "$$\n",
    "\n",
    "* 要对曲线进行变换运算，类似于绘制等高线。\n",
    "\n",
    "![pic](pics/level_set.png \"Level Sets\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed80f8d",
   "metadata": {},
   "source": [
    "# 变分法（Calculus of Variation）\n",
    "&emsp;&emsp;图像降噪和图像增强方面应用较多，扩展求函数极值的方法，主要通过欧拉-拉格朗日微分方程（Euler-Lagrange Differential Equation）主动通过梯度下降过程调整函数自身以满足需求，而不是直接计算出目标点。\n",
    "\n",
    "![pic](pics/calculus_of_variation.png \"Calculus of Variation\")\n",
    "\n",
    "$$\n",
    "\\int F(u, u_x)dx\n",
    "\\\\\n",
    "(\\cfrac{\\partial}{\\partial u} - \\cfrac{d}{dx}\\cfrac{\\partial}{\\partial u_x}) F(u, u_x) = 0\n",
    "$$\n",
    "\n",
    "![pic](pics/euler_lagrange.png \"Example of Euler-Lagrange\")\n",
    "\n",
    "## 各向异性扩散（Anisotropic Diffusion）\n",
    "&emsp;&emsp;高斯模糊是一种各向同性扩散，忽略图像中的边界，而各向异性扩散考虑图像物体中的边界，在边界的一侧对像素值进行平均，本身就是一种去噪声或图像增强的过程。\n",
    "\n",
    "![pic](pics/isotropic_anisotropic.png \"Isotropic and Anisotropic\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ddc758a",
   "metadata": {},
   "source": [
    "# 应用例子\n",
    "&emsp;&emsp;首先基于边缘检测，活动轮廓的目的是为了整合局部的边，以形成一个完整的轮廓。\n",
    "\n",
    "* 一个“好”的轮廓，其轮廓的积分应足够小。\n",
    "\n",
    "![pic](pics/active_contours.png \"Active Contours\")\n",
    "\n",
    "* 根据欧拉-拉格朗日方程进行收缩。\n",
    "\n",
    "![pic](pics/euler_lagrange_exp.png \"example of Euler-Lagrange\")\n",
    "\n",
    "* 通过概率函数（一维）识别边缘。\n",
    "\n",
    "![pic](pics/exp_3.png)\n",
    "\n",
    "* 通过微分几何曲线演化（二维）识别边缘。\n",
    "\n",
    "![pic](pics/exp_4.png)\n",
    "\n",
    "* 直方图拟合以增强图像，让过程可中断，而非直接使用索引表。\n",
    "\n",
    "![pic](pics/exp_5.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dc7960e",
   "metadata": {},
   "source": [
    "Part 2：机器学习——LR 优化\n",
    "======================================\n",
    "\n",
    "* 通用模型  \n",
    "![pic](pics/neuralNetworkBaseFrame.png \"通用神经网络模型框架\")  \n",
    "\n",
    "&emsp;&emsp;由输入 $x_t$ 计算出预测的结果值 $y_t$，进而与标记数据 $\\hat{y}_t$ 对比，计算 Loss  \n",
    "\n",
    "* 符号含义：  \n",
    "    * $\\theta_t$：第 t 步时的模型参数。  \n",
    "    * $\\nabla L(\\theta_t)$ or $g_t$：Loss，$\\theta_t$ 时的梯度，用于计算 $\\theta_{t+1}$。  \n",
    "    * $m_{t+1}$：概括记录从 第 $0$ 步到第 $t$ 步梯度的历史趋势，用于计算 $\\theta_{t+1}$。  \n",
    "\n",
    "\n",
    "* 优化的目的是为了找到可以使 Loss 最小的参数 $\\theta$。  \n",
    "\n",
    "\n",
    "* 训练数据处理有两种方式：暂时假设一次可以获取所有资料，使用 Off-line 方式  \n",
    "    * On-line：一次只处理一对 $(x_t,\\hat y_t)$ 数据  \n",
    "    * Off-line：一次并行处理整组 $(x_t,\\hat y_t)$ 数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6991b2",
   "metadata": {},
   "source": [
    "# 五种已知算法\n",
    "&emsp;&emsp; Adagrad, RMSProp, Adam 属于 Adaptive Learning Rate。  \n",
    "\n",
    "* 相较固定的学习率，采用动态学习率可以训练更快。\n",
    "\n",
    "## SGD(Stochastic Gradient Descent)\n",
    "\n",
    "$$\n",
    "\\theta^{t+1} = \\theta^t - \\eta \\nabla L (\\theta^t)\n",
    "$$\n",
    "\n",
    "\n",
    "![pic](pics/SGD.png \"计算步骤\")  \n",
    "\n",
    "\n",
    "## SGDM(SGD with momentum)\n",
    "&emsp;&emsp;相比 SGD 加入 Momentum 把过去的梯度和算入考量，避免陷入局部梯度为 0 而非最优的情况。  \n",
    "\n",
    "$$\n",
    "v^{t+1} = \\lambda v^t - \\eta \\nabla (\\theta^t) ,\\quad \\theta^{t+1} = \\theta^t + v^{t+1}\n",
    "$$\n",
    "\n",
    "![pic](pics/SGDM.png \"计算步骤\")  \n",
    "&emsp;&emsp;$v^t$ 表示第 $t$ 步 Momentum。  \n",
    "\n",
    "\n",
    "## Adagrad\n",
    "&emsp;&emsp;利用泰勒展开计算得到最优解，同时避免突变。  \n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\sum_{i = 0}^{t}(g_i)^2}} g_t\n",
    "$$\n",
    "\n",
    "![pic](pics/Adagrad.png \"计算步骤\")  \n",
    "\n",
    "\n",
    "## RMSProp\n",
    "&emsp;&emsp;防止 Adagrad 中 Momentum 无止境地变大，但仍无法解决 SGD 卡在梯度为 0 处的问题。  \n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{v_{t+1}}} g_t ,\\quad v_{t+1} = \\alpha v_t + (1 - \\alpha)g_t^2\n",
    "$$\n",
    "\n",
    "\n",
    "## Adam\n",
    "&emsp;&emsp;结合 SGDM 与 RMSProp。\n",
    "\n",
    "$$\n",
    "\\theta_{t+1} = \\theta_t - \\frac{\\eta}{\\sqrt{\\hat{v}_{t+1}}+\\epsilon} \\hat{m}_{t+1}\n",
    "\\\\\n",
    "\\hat{m}_{t+1} = \\frac{m_{t+1}}{1 - \\beta_1^{t+1}} ,\\quad \\hat{v}_{t+1} = \\frac{v_{t+1}}{1 - \\beta_2^{t+1}}\n",
    "\\\\\n",
    "\\beta_1 = 0.9 ,\\quad \\beta_2 = 0.999 ,\\quad \\epsilon = 10^{-8}\n",
    "$$\n",
    "&emsp;&emsp;$m$ 随着时间趋向稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7507faf2",
   "metadata": {},
   "source": [
    "# 发展\n",
    "&emsp;&emsp;自 2014 年以来，没有算法可以显著地超过 Adam 或 SGDM。  \n",
    "\n",
    "* Adam 训练速度更快，但泛化性能较差，不稳定。  \n",
    "\n",
    "\n",
    "* SGDM 更稳定，泛化性能更好，收敛更优。\n",
    "\n",
    "## SWATS \n",
    "> [Keskar, et al., arXiv'17]  \n",
    "\n",
    "一开始用 Adam，定点切换成 SGDM。\n",
    "\n",
    "![pic](pics/SWATS.png \"流程\")  \n",
    "\n",
    "\n",
    "* 如何让 Adam 与 SGDM 一样收敛得更好？  \n",
    "&emsp;&emsp;由 $v_{t+1} = \\beta_2 v_t + (1 - \\beta_2)g_t^2 ,\\quad \\beta_2 = 0.999$ 可知 $v_t$ 会维持约 1000 步的影响，在训练到末尾时大部分梯度非常小，对于参数贡献极少。\n",
    "\n",
    "## Adam 改进\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65480698",
   "metadata": {},
   "source": [
    "### AMSGrad\n",
    "> [Reddi, et al., ICLR'18]  \n",
    "\n",
    "&emsp;&emsp;移除较小的梯度的影响，一直记录较大的梯度，与 Adam 类似，加入 $\\hat{v}_t = max(\\hat{v}_{t-1},v_t)$，但会使学习率越来越小，因此仍不是好的解法，只解决了梯度小时，学习率过大的问题。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11ca8ea0",
   "metadata": {},
   "source": [
    "### AdaBound\n",
    "> [Luo, et al., ICLR'19]  \n",
    "\n",
    "&emsp;&emsp;解决 AMSGrad 只处理学习率过大的缺陷，采用工程经验参数，能用但不灵活。\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - Clip(\\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon}) \\hat{m}_t\n",
    "\\\\\n",
    "Clip(x) = Clip(x,0.1 - \\frac{0.1}{(1 - \\beta_2)t + 1}, 0.1 + \\frac{0.1}{(1 - \\beta_2)t})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed01cdc",
   "metadata": {},
   "source": [
    "## SGDM 改进\n",
    "\n",
    "* 缺陷主要是慢。\n",
    "\n",
    "* 考虑找到一个“最佳”学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fca9e0",
   "metadata": {},
   "source": [
    "### LR range test\n",
    "> [Smith, WACV'17]\n",
    "\n",
    "&emsp;&emsp;LR 小或大时都不会有最好的性能，一定是适中才最有效。  \n",
    "\n",
    "![pic](pics/LR_range_test.png \"适中最优\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d262bc60",
   "metadata": {},
   "source": [
    "### Cyclical LR\n",
    "> [Smith, WACV'17]\n",
    "\n",
    "&emsp;&emsp;让 LR 呈现周期性变化，让 LR 取决于 LR range test，stepsize 有数个 epoch，让 Loss 摆脱局部极小值。  \n",
    "\n",
    "![pic](pics/cyclical_LR.png \"周期性\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9695ab5f",
   "metadata": {},
   "source": [
    "### SGDR  \n",
    "> [Loshchilov, et al., ICLR'17]  \n",
    "\n",
    "&emsp;&emsp;与上一类似。  \n",
    "\n",
    "![pic](pics/SGDR.png \"jump\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ee72be4",
   "metadata": {},
   "source": [
    "### One-cycle LR\n",
    "> [Smith, et al., arXiv'17]  \n",
    "\n",
    "&emsp;&emsp;加入“预热（warm-up） — 退火（annealing） — 微调（fine-tuning）”过程，训练从头到尾，让 LR 一升一降。  \n",
    "\n",
    "![pic](pics/one_cycle_LR.png \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "295696f0",
   "metadata": {},
   "source": [
    "* Adam 也需要预热：由实验得，一开始训练的时候，初始化的位置未知、梯度混乱，前十个有梯度变形（distorted gradient）的情况出现，因此一开始还不确定的时候走小步一点，让它稳定"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "965fcb62",
   "metadata": {},
   "source": [
    "### RAdam\n",
    "> [liu, et al., ICLR'20]  \n",
    "\n",
    "&emsp;&emsp; 为了一开始防止梯度变形，与 SWATS 相反，一开始用 SGDM，之后用 Adam，假设从 Gaussian 采样，认为 Variance 只与 t 有关。  \n",
    "\n",
    "* $\\rho_t$: Effective Memory Size of EMA  \n",
    "* max memory size $t \\rightarrow \\infty$\n",
    "* $r_t \\approx \\cfrac{Var[\\cfrac{1}{\\hat{v}_\\infty}]}{Var[\\cfrac{1}{\\hat{v}_t}]}$\n",
    "\n",
    "$$\n",
    "\\rho_t = \\rho_\\infty - \\frac{2t\\beta_2^t}{1 - \\beta_2^t}\n",
    ",\\quad\n",
    "\\rho_\\infty = \\frac{2}{1 - \\beta_2} - 1\n",
    "\\\\\n",
    "r_t = \\sqrt{\\cfrac{(\\rho_t - 4)(\\rho_t - 2)\\rho_\\infty}{(\\rho_\\infty - 4)(\\rho_\\infty - 2)\\rho_t}}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\theta_t = \\theta_{t-1} - \\eta \\hat{m}_t ,\\quad & \\rho_t \\le 4\n",
    "\\\\\n",
    "\\theta_t = \\theta_{t-1} - \\cfrac{\\eta r_t}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t ,\\quad & \\rho_t \\gt 4\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c99de52c",
   "metadata": {},
   "source": [
    "### RAdam 与 SWATS\n",
    "\n",
    "| |RAdam|SWATS|\n",
    "|:-|:--------|:---------|\n",
    "|动机（Inspiration）|梯度变形使得动态 LR 在一开始不准|Adam 的不收敛与 SGDM 的性能差|\n",
    "|实现（How？）|一开始预热以减少梯度变形|结合两者优点，让训练更快|\n",
    "|转换（Switch）|SGDM to RAdam|Adam to SGDM|\n",
    "|原因（Why switch）|一开始的 $\\hat{v}_t$ 估计无效|让收敛更快|\n",
    "|切换点（Switch point）|估计有效时|人工定义|\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0503064",
   "metadata": {},
   "source": [
    "### Lookahead\n",
    "> [Zhang, et al., arXiv'19]  \n",
    "\n",
    "&emsp;&emsp;每走 k 步，退 1 步，与 MEMO 类似，避免进入局部极小，更好的泛化性能。\n",
    "\n",
    "$For \\, t = 1, 2, ...$  \n",
    "&emsp;&emsp;$\\theta_{t,0} = \\Phi_{t-1}$  \n",
    "&emsp;&emsp;$For \\, i = 1, 2, ...k$  \n",
    "&emsp;&emsp;&emsp;&emsp;$\\theta{t,i} = \\theta{t,i-1} + Optim(Loss, data, \\theta_{t,i-1})$  \n",
    "&emsp;&emsp;$\\Phi_t = \\Phi{t-1} + \\alpha(\\Phi_{t,k} - \\Phi_{t-1})$  \n",
    "\n",
    "![pic](pics/lookahead.png \"k step forward, 1 step back\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57e6725f",
   "metadata": {},
   "source": [
    "### NAG(Nesterov Accelerated Gradient)\n",
    "> [Nesterov, jour Dokl. Akad. Nauk SSSR'83]  \n",
    "\n",
    "&emsp;&emsp;基于 SGDM，提前看结果，再决定当前这一步。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_t\n",
    "& = \\theta_{t-1} - m_t\n",
    "\\\\\n",
    "m_t \n",
    "& = \\lambda m_{t-1} + \\eta \\nabla L (\\theta_{t-1} - \\lambda m_{t-1})\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* 其实不需要记忆多一份参数（？）\n",
    "\n",
    "$$\n",
    "\\theta_t - \\lambda m_t \\Rightarrow \\theta_t' = \\, ... \\, = \\theta_{t-1}' - \\lambda \\color{Red}{m_t} - \\eta \\nabla L (\\theta_{t-1}')\n",
    "\\\\\n",
    "m_t = \\lambda m_{t-1} + \\eta \\nabla L (\\theta_{t-1}')\n",
    "$$\n",
    "\n",
    "* 而 SGDM 有：\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\lambda \\color{Red}{m_{t-1}} - \\eta \\nabla L (\\theta_{t-1})\n",
    "$$\n",
    "\n",
    "* 比较而言相当于用下一步的 $m_t$ 替换成当前步的 $m_t$，超前部署。  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fbdab4",
   "metadata": {},
   "source": [
    "### Nadam\n",
    "> [Dozat, ICLR workshop'16]  \n",
    "\n",
    "&emsp;&emsp;与 NAG 类似进行超前部署。\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\theta_t & = \\theta_{t-1} - \\frac{\\eta}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t\n",
    "\\\\\n",
    "\\hat{m}_t & = \\frac{\\beta_1 \\color{Red}{m_t}}{1 - \\beta_1^{t+1}} + \\frac{(1 - \\beta_1)g_{t-1}}{1 - \\beta_1^t}\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "* 其他部分与 SGDM 一致。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d5d871",
   "metadata": {},
   "source": [
    "### L2 Regularization\n",
    "\n",
    "$$\n",
    "L_{l_2}(\\theta) = L(\\theta) \\color{Red}{+ \\frac{1}{2} \\gamma ||\\theta||^2}\n",
    "$$\n",
    "\n",
    "* SGD:  \n",
    "$$\n",
    "\\theta_t \n",
    "= \\theta_{t-1} - \\nabla L_{l_2}(\\theta_{t-1}) \n",
    "= \\theta_{t-1} - \\nabla L(\\theta_{t-1}) \\color{Red}{- \\gamma \\theta_{t-1}}\n",
    "$$\n",
    "\n",
    "* SGDM:  \n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - \\lambda m_{t-1} - \\eta(\\nabla L(\\theta_{t-1}) \\color{Red}{+ \\gamma \\theta_{t-1}})\n",
    "\\\\\n",
    "m_t = \\lambda m_{t-1} + \\eta (\\nabla L (\\theta_{t-1}) \\color{Red}{+ \\gamma \\theta_{t-1}}) \\, ?\n",
    "\\\\\n",
    "m_t = \\lambda m_{t-1} + \\eta (\\nabla L (\\theta_{t-1})) \\, ?\n",
    "$$\n",
    "\n",
    "* Adam:\n",
    "\n",
    "$$\n",
    "m_t = \\lambda m_{t-1} + \\eta (\\nabla L (\\theta_{t-1}) \\color{Red}{+ \\gamma \\theta_{t-1})} \\, ?\n",
    "\\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L (\\theta_{t-1}) \\color{Red}{+ \\gamma \\theta_{t-1})^2} \\, ?\n",
    "$$\n",
    "\n",
    "* Gradient 是否应该在 Momentum 或 LR 的分母项加上最小二乘项？  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34698685",
   "metadata": {},
   "source": [
    "### AdamW & SGDW with momentum\n",
    "> [Loshchilov, arXiv'17]  \n",
    "\n",
    "&emsp;&emsp;W 指 weight decay，不加最小二乘项比较好，但仍然在计算参数项里减去 $\\gamma \\theta_{t-1}$ 以达到 Regularization 的效果。  \n",
    "\n",
    "* SGDWM:\n",
    "\n",
    "$$\n",
    "\\theta_t = \\theta_{t-1} - m_t - \\gamma \\theta_{t-1}\n",
    "\\\\\n",
    "m_t = \\lambda m_{t-1} + \\eta (\\nabla L (\\theta_{t-1}))\n",
    "$$\n",
    "\n",
    "* AdamW:\n",
    "\n",
    "$$\n",
    "m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla L (\\theta_{t-1})\n",
    "\\\\\n",
    "v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla L (\\theta_{t-1})^2\n",
    "\\\\\n",
    "\\theta_t = \\theta_{t-1} - \\eta(\\frac{1}{\\sqrt{\\hat{v}_t} + \\epsilon} \\hat{m}_t - \\gamma \\theta_{t-1})\n",
    "$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84e3ab97",
   "metadata": {},
   "source": [
    "# Additional Optimization Method\n",
    "* Shuffling：增加 Data 随机性。\n",
    "\n",
    "\n",
    "* Dropout：鼓励每一个 network 学习到有意义的资讯，也作为增加随机性的方法。\n",
    "\n",
    "\n",
    "* Gradient noise：算完 Gradient 后加入 Gaussian Noise，增加 Model 的随机性。\n",
    "> [Neelakantan, et al., arXiv'15]\n",
    "\n",
    "$$\n",
    "g_{t,i} = g_{t,i} + N(0, \\sigma_t^2)\n",
    "\\\\\n",
    "\\sigma_t = \\frac{c}{(1+t)^\\gamma}\n",
    "$$\n",
    "\n",
    "* Warm-up：一开始训练能力较差，等训练能力稳定后调大 LR。\n",
    "\n",
    "\n",
    "* Curriculum learning：一开始先用简单 Data，等 Model 足够健壮后再使用较难 Data。\n",
    "> [Bengio, et al., ICML'09]\n",
    "\n",
    "&emsp;&emsp;一开始揠苗助长，开始的学习决定 Model 大方向走向，学习一段时间后会进入 local minimum，一开始使用靠近平均的 Data\n",
    "\n",
    "\n",
    "* Fine-tuning：使用已经训练好的模型，避免浪费资源。\n",
    "\n",
    "\n",
    "* Normalization：防止 Model 学习到太极端。\n",
    "\n",
    "\n",
    "* Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87d7038b",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "&emsp;&emsp;并不存在一个通用的 Optimizer，Adam 还是比较常用。\n",
    "\n",
    "## 分类\n",
    "\n",
    "![pic](pics/con1.png \"分类\")\n",
    "\n",
    "## 特点\n",
    "\n",
    "![pic](pics/con2.png \"特点\")\n",
    "\n",
    "## 训练领域建议\n",
    "\n",
    "![pic](pics/con3.png \"研究方向\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17042158",
   "metadata": {},
   "source": [
    "# pytorch 框架完成李宏毅课程第一次作业"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8271907",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.11.0+cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "print(torch.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db1a779",
   "metadata": {},
   "source": [
    "* 取得数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "23d34bc9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shape : ( 4320 , 24 )\n"
     ]
    }
   ],
   "source": [
    "feature_num = 18  # 已知 feature 有 18 个\n",
    "data_num = 12 * 20 * 24  # 12 个月，每月 20 天，每天 24 小时\n",
    "train_data = []  # 包含所有 example 列的列表\n",
    "\n",
    "train_file = open(\"data/train.csv\", 'r', encoding='big5')  # 已知编码格式为 big5\n",
    "\n",
    "next(train_file)  # 掐头\n",
    "count = 0\n",
    "for line in train_file:\n",
    "    cur_line = line.strip().split(',')[3:]  # 继续掐头\n",
    "    \n",
    "    if count % 18 == 10:  # 处理 RAINFALL 行\n",
    "        toline = []\n",
    "        for exp in cur_line:\n",
    "            if exp == 'NR':\n",
    "                toline.append(0)\n",
    "            else:\n",
    "                toline.append(float(exp))\n",
    "    else:\n",
    "        toline = [float(x) for x in cur_line]\n",
    "    \n",
    "    train_data.append(toline)\n",
    "    count += 1\n",
    "\n",
    "train_file.close()\n",
    "\n",
    "print('shape :', '(', len(train_data), ',', len(train_data[0]), ')')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8623fc42",
   "metadata": {},
   "source": [
    "* 已知 4320 对应为 12 个月，20 天，18 个 feature，24 对应为 24 小时\n",
    "* 将其重组分成 12 * 18 * 480 h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "b23ea759",
   "metadata": {},
   "outputs": [],
   "source": [
    "month_data = []\n",
    "\n",
    "for month in range(12):\n",
    "    cur_month = []\n",
    "    for feature in range(18):\n",
    "        cur_feature = []\n",
    "        for day in range(20):\n",
    "            cur_feature += train_data[20 * month + 18 * day + feature]\n",
    "        cur_month.append(cur_feature)\n",
    "    month_data.append(cur_month)\n",
    "month_data = np.array(month_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4395c263",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12, 18, 480)\n",
      "[14.  14.  14.  13.  12.  12.  12.  12.  15.  17.  20.  22.  22.  22.\n",
      " 22.  22.  21.  19.  17.  16.  15.  15.  15.  15.  16.  15.  15.  14.\n",
      " 14.  15.  16.  16.  17.  20.  22.  23.  24.  24.  24.  24.  23.  21.\n",
      " 20.  19.  18.  18.  18.  18.  18.  18.  18.  18.  18.  18.  18.  19.\n",
      " 19.  20.  22.  23.  24.  24.  24.  22.  22.  21.  20.  20.  20.  20.\n",
      " 20.  19.  19.  18.  18.  17.  17.  16.  17.  17.  18.  18.  19.  19.\n",
      " 19.  20.  19.  18.  18.  16.  16.  15.  15.  15.  14.  13.  13.  12.\n",
      " 13.  12.  12.  12.  12.  13.  14.  16.  18.  19.  20.  21.  21.  20.\n",
      " 19.  17.  16.  16.  15.  14.  14.  13.  13.  13.  13.  14.  14.  15.\n",
      " 14.  14.  16.  16.  18.  19.  17.  20.  20.  19.  18.  17.  16.  15.\n",
      " 15.  15.  15.  14.  14.  14.  14.  14.  14.  14.  14.  14.  16.  18.\n",
      " 21.  22.  23.  23.  20.  19.  20.  19.  18.  19.  17.  16.  16.  16.\n",
      " 15.  15.  15.  15.  15.  15.  15.  16.  18.  21.  22.  23.  23.  23.\n",
      " 22.  20.  19.  18.  18.  17.  16.  15.  14.  14.  14.  13.  13.  13.\n",
      " 12.  12.  12.  13.  13.  14.  14.  14.  15.  15.  15.  14.  13.  12.\n",
      " 12.  12.  12.  12.  12.  12.  12.  12.  13.  12.  13.  13.  13.  13.\n",
      " 14.  15.  17.  18.  19.  20.  20.  19.  18.  17.  15.  14.  14.  14.\n",
      " 13.  13.  13.  13.  13.  13.  13.  12.  12.  12.  15.  18.  20.  21.\n",
      " 21.  20.  19.  19.  19.  18.  17.  16.  16.  16.  16.  17.  17.  17.\n",
      " 17.  16.  16.  16.  15.  14.  15.  17.  17.  18.  19.  20.  20.  19.\n",
      " 18.  16.  15.  15.  15.  15.  15.  15.  15.  14.  14.  14.  14.  14.\n",
      " 13.  13.  14.  15.  16.  17.  18.  18.  17.  15.  15.  14.  14.  14.\n",
      " 14.  14.  14.  14.  13.  14.  14.  14.  14.  14.  13.  13.  14.  14.\n",
      " 15.  15.  14.  14.  13.  13.  13.  12.  12.  12.  12.  12.  12.  11.\n",
      " 11.  11.  11.  11.  11.  10.  11.  11.  12.  14.  16.  17.  18.  18.\n",
      " 18.  18.  17.  15.  14.  14.  13.  12.  13.  13.  13.  13.  14.  13.\n",
      " 12.  12.  12.  12.  13.  15.  16.  18.  20.  20.  20.  20.  19.  18.\n",
      " 16.  15.  14.  14.  13.  13.  13.  13.  13.  12.  12.  12.  12.  13.\n",
      " 15.  17.  19.  20.  21.  21.  20.  20.  19.  17.  16.  15.  15.  14.\n",
      " 14.  13.  13.  12.  12.  12.  13.  13.  12.  12.  13.  15.  16.  17.\n",
      " 18.  17.  17.  15.  14.  13.  11.  10.   9.9  9.9  9.6  9.3  9.6  9.8\n",
      " 10.  10.   9.7 10.  10.  10.  12.  14.  17.  18.  18.  19.  19.  18.\n",
      " 17.  17.  15.  15.  15.  15.  14.  13.  12.  12.  11.  12.  12.  11.\n",
      " 10.  10.  12.  15.  16.  17.  18.  18.  18.  18.  17.  15.  15.  14.\n",
      " 14.  14.  13.  13. ]\n"
     ]
    }
   ],
   "source": [
    "# print(\"shape :\", '(', len(month_data), ',', len(month_data[0]), ',', len(month_data[0][0]), ')')\n",
    "# print(month_data[0][0])\n",
    "print(month_data.shape)\n",
    "print(month_data[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58f619ff",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c8cab48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b816dbae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2351ce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "350c0c45",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cdb2fa46",
   "metadata": {},
   "source": [
    "Reference:   \n",
    "================================================  \n",
    "<font face=\"Times New Roman\">\n",
    "\n",
    "Gonzalez, C., Woods, E. (2020) *Digital Image Processing*, Fourth Edition, Beijing, Electronics Industry  \n",
    "Aston Zhang and Zachary C. Lipton and Mu Li and Alexander J. Smola (2020) *Dive into Deep Learning* [online]. Available from: http://www.d2l.ai [Accessed: 18th July 2022].  \n",
    "\n",
    "<font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
